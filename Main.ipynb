{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b8c34-e53e-46ee-822d-4db296ca1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "pip install requests wikipedia-api langchain langchain-core langgraph smolagents huggingface-hub chromadb sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862c697-2eb8-496b-859c-2098da69b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-core langchain-community langgraph chromadb sentence-transformers wikipedia-api smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b30a15-f32f-469d-85f5-2c0e14931443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import requests\n",
    "import wikipediaapi\n",
    "\n",
    "import re\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from tavily import TavilyClient\n",
    "from typing import TypedDict\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from smolagents import InferenceClientModel\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7611d04-57c0-4f0f-adaa-f8bc83ce45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch evaluation questions from Hugging Face Agents Course API\n",
    "import requests\n",
    "BASE_URL = \"https://agents-course-unit4-scoring.hf.space\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c057c1-62b5-40cd-8085-48b981cd2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all evaluation questions\n",
    "resp = requests.get(f\"{BASE_URL}/questions\")\n",
    "questions = resp.json()\n",
    "\n",
    "print(\"Number of questions:\", len(questions))\n",
    "print(\"Example question:\")\n",
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e543368-3cce-4471-ad9b-1025d46f584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a random evaluation question from the API\n",
    "\n",
    "BASE_URL = \"https://agents-course-unit4-scoring.hf.space\"\n",
    "\n",
    "resp = requests.get(f\"{BASE_URL}/random-question\")\n",
    "random_question = resp.json()\n",
    "\n",
    "# Print the random question\n",
    "print(\"Random Question:\", random_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1ca70-7591-4e97-8315-35ce3001f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main LLM\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "\n",
    "client = InferenceClient(token=os.environ[\"HF_TOKEN\"])\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly AI.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you today?\"}\n",
    "]\n",
    "\n",
    "output = client.chat_completion(\n",
    "    model=model_id,\n",
    "    messages=messages,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(output.choices[0].message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98654335-94ea-45f5-9a04-b9101ecfc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a  Wikipedia-based knowledge base\n",
    "import wikipediaapi\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Initialize Wikipedia API client\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    user_agent='GAIA-Agent'  \n",
    ")\n",
    "\n",
    "# List of Wikipedia topics to include in the knowledge base\n",
    "topics = [\n",
    "    \"Mercedes Sosa discography\", \"1928 Summer Olympics\", \"Ada Lovelace\", \"Moon landing\",\n",
    "    \"Olympic Games medal table\", \"List of studio albums by Mercedes Sosa\", \"FIFA World Cup winners\",\n",
    "    \"List of countries by population\", \"List of Nobel laureates\", \"Marie Curie\",\n",
    "    \"Albert Einstein\", \"Isaac Newton\", \"Apollo 11\", \"First man in space\",\n",
    "    \"Solar System\", \"Periodic table\", \"Human anatomy\", \"COVID-19 pandemic\",\n",
    "    \"United Nations\", \"History of the Internet\", \"Artificial intelligence\"\n",
    "]\n",
    "\n",
    "# Fetch Wikipedia content and convert to LangChain Document objects\n",
    "docs = []\n",
    "for topic in topics:\n",
    "    page = wiki_wiki.page(topic)\n",
    "    if page.exists():\n",
    "        for para in page.text.split(\"\\n\"):\n",
    "            if para.strip():\n",
    "                docs.append(Document(page_content=para.strip(), metadata={\"topic\": topic}))\n",
    "\n",
    "print(f\"Collected {len(docs)} Wikipedia documents for GAIA knowledge base.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c37f8-75bd-4c1d-a157-c76224f56dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a retriever agent using Chroma and SentenceTransformers\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Create vector store from Wikipedia documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "# Convert vector store to a retriever for semantic search\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67349a-a591-465c-a5a1-be5812edbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION Tavily\n",
    "# -------------------------------\n",
    "\n",
    "# Example: os.environ[\"TAVILY_API_KEY\"] = \"api_key_here\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"API-KEY\"\n",
    "\n",
    "# Initialize Tavily client\n",
    "try:\n",
    "    if \"TAVILY_API_KEY\" in os.environ and os.environ[\"TAVILY_API_KEY\"].startswith(\"tvly-\"):\n",
    "        tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "        print(\"Tavily Client Initialized.\")\n",
    "    else:\n",
    "        tavily = None\n",
    "        print(\"Tavily API Key not properly configured.\")\n",
    "except Exception as e:\n",
    "    tavily = None\n",
    "    print(f\"Error initializing Tavily: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca5342-d3f6-46cd-bae4-45b2a8e9ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# DEFINE LANGGRAPH STATE\n",
    "# -------------------------------\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[AnyMessage]\n",
    "    context: str\n",
    "    tool_used: bool  # Track if Tavily search already used\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# RETRIEVER NODE (Internal Search)\n",
    "# -------------------------------\n",
    "def retriever_node(state: AgentState) -> AgentState:\n",
    "    global retriever\n",
    "    print(\"NODE: Running RAG (Internal Retrieval)\")\n",
    "\n",
    "    question = state[\"messages\"][-1].content\n",
    "    try:\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "    except Exception as e:\n",
    "        print(f\"Retriever failed: {e}\")\n",
    "        docs = []\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs[:3]]) if docs else \"\"\n",
    "    return {\"context\": context, \"tool_used\": state.get(\"tool_used\", False)}\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# REASONING NODE (Decision + LLM)\n",
    "# -------------------------------\n",
    "def reasoning_node(state: AgentState) -> AgentState:\n",
    "    global model\n",
    "    print(\"NODE: Running Reasoning and Routing\")\n",
    "\n",
    "    question = state[\"messages\"][-1].content\n",
    "    context = state.get(\"context\", \"\")\n",
    "\n",
    "    truncated_context = context[:1000]  # avoid long LLM prompts\n",
    "\n",
    "    full_prompt = (\n",
    "        f\"You are a helpful agent. Your goal is to answer the user's question.\\n\"\n",
    "        f\"1. If CONTEXT below is enough, respond with 'FINAL ANSWER: ...'\\n\"\n",
    "        f\"2. If CONTEXT is empty and no tool used, respond with 'TOOL CALL: <query>'.\\n\"\n",
    "        f\"3. If context contains Tavily info, give a FINAL ANSWER.\\n\\n\"\n",
    "        f\"CONTEXT:\\n{truncated_context}\\n\\nQUESTION: {question}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    huggingface_messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    try:\n",
    "        model_output = model(huggingface_messages)\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed: {e}\")\n",
    "        model_output = [{\"generated_text\": \"TOOL CALL: \" + question}]\n",
    "\n",
    "    if isinstance(model_output, list) and model_output and isinstance(model_output[0], dict):\n",
    "        ai_message_content = model_output[0].get(\"generated_text\", \"No generated text found.\")\n",
    "    elif isinstance(model_output, dict):\n",
    "        ai_message_content = model_output.get(\"generated_text\", \"No generated text found.\")\n",
    "    else:\n",
    "        ai_message_content = str(model_output)\n",
    "\n",
    "    ai_message = AIMessage(content=ai_message_content)\n",
    "    new_messages = state[\"messages\"] + [ai_message]\n",
    "\n",
    "    return {\"messages\": new_messages, \"context\": context, \"tool_used\": state.get(\"tool_used\", False)}\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# TOOL EXECUTOR NODE (Tavily Search)\n",
    "# -------------------------------\n",
    "def tool_executor_node(state: AgentState) -> AgentState:\n",
    "    global tavily\n",
    "    print(\"NODE: Executing Tavily Web Search\")\n",
    "\n",
    "    if not tavily:\n",
    "        return {\"context\": \"Error: Tavily not initialized.\", \"tool_used\": True}\n",
    "\n",
    "    ai_message = state[\"messages\"][-1].content\n",
    "    match = re.search(r\"TOOL CALL:\\s*(.*)\", ai_message, re.IGNORECASE | re.DOTALL)\n",
    "    if not match:\n",
    "        print(\"TOOL NODE ERROR: Could not parse query.\")\n",
    "        return {\"context\": \"Error parsing tool call.\", \"tool_used\": True}\n",
    "\n",
    "    query = match.group(1).strip()\n",
    "    print(f\"Tavily Search Query: '{query}'\")\n",
    "\n",
    "    try:\n",
    "        response = tavily.search(query=query, search_depth=\"basic\", max_results=5)\n",
    "        search_results = [\n",
    "            f\"Source: {r['url']}\\nContent: {r['content']}\"\n",
    "            for r in response['results']\n",
    "        ]\n",
    "        new_context = f\"--- External Info from Tavily ({query}) ---\\n\" + \"\\n---\\n\".join(search_results)\n",
    "    except Exception as e:\n",
    "        print(f\"Tavily search failed: {e}\")\n",
    "        new_context = f\"Error: Tavily search failed. {e}\"\n",
    "\n",
    "    return {\"context\": new_context, \"tool_used\": True}\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# ROUTING LOGIC\n",
    "# -------------------------------\n",
    "def route_to_tool_or_end(state: AgentState) -> str:\n",
    "    llm_output = state[\"messages\"][-1].content.upper()\n",
    "    tool_used = state.get(\"tool_used\", False)\n",
    "\n",
    "    if \"FINAL ANSWER\" in llm_output:\n",
    "        return END\n",
    "    elif \"TOOL CALL\" in llm_output and not tool_used:\n",
    "        return \"tool_executor\"\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf873c7-3290-435c-a9e0-a4633b0718b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# BUILD AND COMPILE GRAPH\n",
    "# -------------------------------\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"retriever\", retriever_node)\n",
    "workflow.add_node(\"reasoning\", reasoning_node)\n",
    "workflow.add_node(\"tool_executor\", tool_executor_node)\n",
    "workflow.add_edge(START, \"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"reasoning\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"reasoning\",\n",
    "    route_to_tool_or_end,\n",
    "    {\"tool_executor\": \"tool_executor\", END: END}\n",
    ")\n",
    "workflow.add_edge(\"tool_executor\", \"reasoning\")\n",
    "\n",
    "app = workflow.compile()\n",
    "print(\"Agent Graph Compiled Successfully with Tavily integration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053b904-b273-4a9d-8545-cfd3a1afd86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch GAIA questions\n",
    "BASE_URL = \"https://agents-course-unit4-scoring.hf.space\"\n",
    "resp = requests.get(f\"{BASE_URL}/questions\")\n",
    "questions = resp.json()\n",
    "print(f\"Fetched {len(questions)} GAIA questions.\")\n",
    "print(\"Example question:\", questions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3ed07-0a7b-496b-8c8b-82f801c1d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXECUTION BLOCK FOR GAIA QUESTIONS\n",
    "\n",
    "answers = []\n",
    "for q in questions:\n",
    "    task_id = q[\"task_id\"]\n",
    "    question_text = q[\"question\"]\n",
    "\n",
    "    # Initialize agent state\n",
    "    state_input = {\n",
    "        \"messages\": [HumanMessage(content=question_text)],\n",
    "        \"context\": \"\",\n",
    "        \"tool_used\": False\n",
    "    }\n",
    "\n",
    "    # Invoke the agent\n",
    "    result = app.invoke(state_input)\n",
    "\n",
    "    # Extract final message\n",
    "    final_message_content = result[\"messages\"][-1].content\n",
    "\n",
    "    # Determine source\n",
    "    context_used = result.get(\"context\", \"\")\n",
    "    if \"--- External Information from Tavily Web Search\" in context_used:\n",
    "        source = \"Tool (Tavily Search)\"\n",
    "    elif final_message_content.startswith(\"FINAL ANSWER\"):\n",
    "        source = \"Wikipedia RAG\"\n",
    "    else:\n",
    "        source = \"Unknown\"\n",
    "\n",
    "    # Print GAIA question, answer, and source\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"GAIA Question [{task_id}]: {question_text}\")\n",
    "    print(\"Final Answer:\", final_message_content)\n",
    "    print(\"Source:\", source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52685dbc-83eb-4a30-93e2-ada33140db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Save for submission\n",
    "    answers.append({\n",
    "        \"task_id\": task_id,\n",
    "        \"submitted_answer\": final_message_content,\n",
    "        \"source\": source\n",
    "    })\n",
    "\n",
    "print(\"\\nAll GAIA answers processed and ready for submission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c0714-129e-423d-a095-7983f54d0dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
